
@inproceedings{laban_beyond_2024,
	address = {Pittsburgh PA USA},
	title = {Beyond the {Chat}: {Executable} and {Verifiable} {Text}-{Editing} with {LLMs}},
	isbn = {9798400706288},
	shorttitle = {Beyond the {Chat}},
	url = {https://dl.acm.org/doi/10.1145/3654777.3676419},
	doi = {10.1145/3654777.3676419},
	language = {en},
	urldate = {2024-10-27},
	booktitle = {Proceedings of the 37th {Annual} {ACM} {Symposium} on {User} {Interface} {Software} and {Technology}},
	publisher = {ACM},
	author = {Laban, Philippe and Vig, Jesse and Hearst, Marti and Xiong, Caiming and Wu, Chien-Sheng},
	month = oct,
	year = {2024},
	pages = {1--23},
}

@article{pozdniakov_large_2024,
	title = {Large language models meet user interfaces: {The} case of provisioning feedback},
	volume = {7},
	issn = {2666-920X},
	shorttitle = {Large language models meet user interfaces},
	url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000924},
	doi = {10.1016/j.caeai.2024.100289},
	abstract = {Incorporating Generative Artificial Intelligence (GenAI), especially Large Language Models (LLMs), into educational settings presents valuable opportunities to boost the efficiency of educators and enrich the learning experiences of students. A significant portion of the current use of LLMs by educators has involved using conversational user interfaces (CUIs), such as chat windows, for functions like generating educational materials or offering feedback to learners. The ability to engage in real-time conversations with LLMs, which can enhance educators' domain knowledge across various subjects, has been of high value. However, it also presents challenges to LLMs' widespread, ethical, and effective adoption. Firstly, educators must have a degree of expertise, including tool familiarity, AI literacy and prompting to effectively use CUIs, which can be a barrier to adoption. Secondly, the open-ended design of CUIs makes them exceptionally powerful, which raises ethical concerns, particularly when used for high-stakes decisions like grading. Additionally, there are risks related to privacy and intellectual property, stemming from the potential unauthorised sharing of sensitive information. Finally, CUIs are designed for short, synchronous interactions and often struggle and hallucinate when given complex, multi-step tasks (e.g., providing individual feedback based on a rubric on a large scale). To address these challenges, we explored the benefits of transitioning away from employing LLMs via CUIs to the creation of applications with user-friendly interfaces that leverage LLMs through API calls. We first propose a framework for pedagogically sound and ethically responsible incorporation of GenAI into educational tools, emphasizing a human-centred design. We then illustrate the application of our framework to the design and implementation of a novel tool called Feedback Copilot, which enables instructors to provide students with personalized qualitative feedback on their assignments in classes of any size. An evaluation involving the generation of feedback from two distinct variations of the Feedback Copilot tool, using numerically graded assignments from 338 students, demonstrates the viability and effectiveness of our approach. Our findings have significant implications for GenAI application researchers, educators seeking to leverage accessible GenAI tools, and educational technologists aiming to transcend the limitations of conversational AI interfaces, thereby charting a course for the future of GenAI in education.},
	urldate = {2024-10-26},
	journal = {Computers and Education: Artificial Intelligence},
	author = {Pozdniakov, Stanislav and Brazil, Jonathan and Abdi, Solmaz and Bakharia, Aneesha and Sadiq, Shazia and Gašević, Dragan and Denny, Paul and Khosravi, Hassan},
	month = dec,
	year = {2024},
	keywords = {Artificial intelligence, Feedback, Generative artificial intelligence, Interfaces, Large language models, Learning analytics},
	pages = {100289},
}

@inproceedings{punter_conducting_2003,
	title = {Conducting on-line surveys in software engineering},
	url = {https://ieeexplore.ieee.org/abstract/document/1237967},
	doi = {10.1109/ISESE.2003.1237967},
	abstract = {One purpose of empirical software engineering is to enable an understanding of factors that influence software development. Surveys are an appropriate empirical strategy to gather data from a large population (e.g., about methods, tools, developers, companies) and to achieve an understanding of that population. Although surveys are quite often performed, for example, in social sciences and marketing research, they are underrepresented in empirical software engineering research, which most often uses controlled experiments and case studies. Consequently, also the methodological support how to perform such studies in software engineering is rather low. However, with the increasing pervasion of the Internet it is possible to perform surveys easily and cost-effectively over Internet pages (i.e., on-line), while at the same time the interest in performing surveys is growing. The purpose of this paper is twofold. First we want to arise the awareness of on-line surveys and discuss methods how to perform these in the context of software engineering. Second, we report our experience in performing on-line surveys in the form of lessons learned and guidelines.},
	urldate = {2024-10-26},
	booktitle = {2003 {International} {Symposium} on {Empirical} {Software} {Engineering}, 2003. {ISESE} 2003. {Proceedings}.},
	author = {Punter, T. and Ciolkowski, M. and Freimut, B. and John, I.},
	month = sep,
	year = {2003},
	keywords = {Context awareness, Costs, Guidelines, Humans, Internet, Programming, Software engineering, Stress},
	pages = {80--88},
}

@article{patkar_challenges_2024,
	title = {Challenges and {Opportunities} for {Prompt} {Management}: {Empirical} {Investigation} of {Text}-based {GenAI} {Users}},
	abstract = {Generative AI (genAI) tools, like ChatGPT, have become popular not only with everyday users but also with Human-Computer Interaction (HCI) researchers and practitioners. Despite their rapid adoption, there is a lack of studies examining their design, particularly regarding prompt handling, organization, and management. Our empirical survey study, involving 61 genAI tool users, addresses this gap by investigating the usability and user experience of the current features of these tools. We illustrate that advanced search and labeling functionalities and innovative interface designs can significantly enhance user experience as well as aid in reflecting on sustainability when using this technology. As genAI approaches the so-called “Trough of Disillusionment” (in Gartner’s Hype Cycle terms),1 our research aims to guide the design of genAI tools toward a more pragmatic and practical fit to end-user practices, ensuring that technology adoption comes with a deeper understanding of its capabilities and offerings.},
	language = {en},
	journal = {. September},
	author = {Patkar, Nitish and Fedosov, Anton and Kropp, Martin},
	year = {2024},
}

@article{shneiderman_direct_1983,
	title = {Direct manipulation: {A} step beyond programming languages},
	volume = {16},
	issn = {0018-9162},
	number = {08},
	journal = {Computer},
	author = {Shneiderman, Ben},
	year = {1983},
	note = {Publisher: IEEE Computer Society},
	pages = {57--69},
}

@article{shneiderman_direct_1983-1,
	title = {Direct manipulation: {A} step beyond programming languages},
	volume = {16},
	number = {08},
	journal = {Computer},
	author = {Shneiderman, Ben},
	year = {1983},
	note = {ISBN: 0018-9162
Publisher: IEEE Computer Society},
	pages = {57--69},
}

@article{shneiderman_direct_1983-2,
	title = {Direct manipulation: {A} step beyond programming languages},
	volume = {16},
	issn = {0018-9162},
	number = {08},
	journal = {Computer},
	author = {Shneiderman, Ben},
	year = {1983},
	note = {Publisher: IEEE Computer Society},
	pages = {57--69},
}

@article{shneiderman_direct_1983-3,
	title = {Direct manipulation: {A} step beyond programming languages},
	volume = {16},
	number = {08},
	journal = {Computer},
	author = {Shneiderman, Ben},
	year = {1983},
	note = {ISBN: 0018-9162
Publisher: IEEE Computer Society},
	pages = {57--69},
}

@article{shneiderman_direct_1983-4,
	title = {Direct manipulation: {A} step beyond programming languages},
	volume = {16},
	issn = {0018-9162},
	number = {08},
	journal = {Computer},
	author = {Shneiderman, Ben},
	year = {1983},
	note = {Publisher: IEEE Computer Society},
	pages = {57--69},
}

@misc{mugunthan_overcoming_2023,
	title = {Overcoming the {Articulation} {Barrier} in {Generative} {AI} {Using} {Hybrid} {Interfaces}},
	url = {https://www.nngroup.com/articles/ai-articulation-barrier/},
	abstract = {Hybrid user interfaces that combine prompt-based inputs with a graphical user interface (GUI) can make AI image-generation tools more usable by reducing users’ cognitive load and improving discoverability.},
	language = {en},
	urldate = {2024-04-01},
	journal = {Nielsen Norman Group},
	author = {Mugunthan, Tarun},
	year = {2023},
}

@misc{openai_best_202,
	title = {Best practices for prompt engineering with the {OpenAI} {API} {\textbar} {OpenAI} {Help} {Center}},
	url = {https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api},
	abstract = {How to give clear and effective instructions to OpenAI models},
	language = {en},
	urldate = {2024-04-01},
	author = {OpenAI},
	year = {202},
}

@misc{dang_how_2022,
	title = {How to {Prompt}? {Opportunities} and {Challenges} of {Zero}- and {Few}-{Shot} {Learning} for {Human}-{AI} {Interaction} in {Creative} {Applications} of {Generative} {Models}},
	shorttitle = {How to {Prompt}?},
	url = {http://arxiv.org/abs/2209.01390},
	doi = {10.48550/arXiv.2209.01390},
	abstract = {Deep generative models have the potential to fundamentally change the way we create high-fidelity digital content but are often hard to control. Prompting a generative model is a promising recent development that in principle enables end-users to creatively leverage zero-shot and few-shot learning to assign new tasks to an AI ad-hoc, simply by writing them down. However, for the majority of end-users writing effective prompts is currently largely a trial and error process. To address this, we discuss the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction. Based on our analysis, we propose four design goals for user interfaces that support prompting. We illustrate these with concrete UI design sketches, focusing on the use case of creative writing. The research community in HCI and AI can take these as starting points to develop adequate user interfaces for models capable of zero- and few-shot learning.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Dang, Hai and Mecke, Lukas and Lehmann, Florian and Goller, Sven and Buschek, Daniel},
	month = sep,
	year = {2022},
	note = {arXiv:2209.01390 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, H.5.2, I.2.7},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2024-04-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing {ChatGPT}},
	url = {https://openai.com/blog/chatgpt},
	abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
	language = {en-US},
	urldate = {2024-04-01},
}

@article{morris_prompt_2023,
	title = {Prompt {Programming} for {Large} {Language} {Models} via {Mixed} {Initiative} {Interaction} in a {GUI}},
	abstract = {Large Language Models (LLMs) now demonstrate many surprising capabilities that previously required special algorithms, for example interactive correction of syntax errors in structured text. However, the problem of how to systematically and reliably access these capabilities of LLMs has led to a new genre of “prompt programming” or “prompt engineering”. This paper presents a design case study in which we apply OpenAI’s Codex to an interface requiring syntax-constrained textual input, an email client. Via a mixed-initiative interface design, the system provides appropriate suggestions based on the output of the LLM. A user study was undertaken, finding that the incorporation of a LLM resulted in a decrease in perceived workload as well as a 62.5\% reduction in errors. This work demonstrates how mixed-initiative interface design can better support attention investment in the use of LLMs, by delivering capabilities that might otherwise require prompt programming in a chat dialogue, but via a relatively conventional GUI.},
	language = {en},
	author = {Morris, Tanya A and Blackwell, Alan F},
	year = {2023},
}

@misc{perplexity_ai_perplexity_2024,
	title = {Perplexity {AI}},
	url = {https://www.perplexity.ai/},
	abstract = {Perplexity AI unlocks the power of knowledge with information discovery and sharing.},
	language = {en},
	urldate = {2024-04-01},
	journal = {Perplexity},
	author = {Perplexity AI},
	year = {2024},
}

@misc{noauthor_perplexity_nodate,
	title = {Perplexity {Blog}},
	url = {https://www.perplexity.ai/hub},
	abstract = {Explore Perplexity's blog for articles, announcements, product updates, and tips to optimize your experience. Stay informed and make the most of Perplexity.},
	language = {en},
	urldate = {2024-04-01},
}

@inproceedings{zamfirescu-pereira_herding_2023,
	address = {New York, NY, USA},
	series = {{DIS} '23},
	title = {Herding {AI} {Cats}: {Lessons} from {Designing} a {Chatbot} by {Prompting} {GPT}-3},
	isbn = {978-1-4503-9893-0},
	shorttitle = {Herding {AI} {Cats}},
	url = {https://dl.acm.org/doi/10.1145/3563657.3596138},
	doi = {10.1145/3563657.3596138},
	abstract = {Prompting Large Language Models (LLMs) is an exciting new approach to designing chatbots. But can it improve LLM’s user experience (UX) reliably enough to power chatbot products? Our attempt to design a robust chatbot by prompting GPT-3/4 alone suggests: not yet. Prompts made achieving “80\%” UX goals easy, but not the remaining 20\%. Fixing the few remaining interaction breakdowns resembled herding cats: We could not address one UX issue or test one design solution at a time; instead, we had to handle everything everywhere all at once. Moreover, because no prompt could make GPT reliably say “I don’t know” when it should, the user-GPT conversations had no guardrails after a breakdown occurred, often leading to UX downward spirals. These risks incentivized us to design highly prescriptive prompts and scripted bots, counter to the promises of LLM-powered chatbots. This paper describes this case study, unpacks prompting’s fickleness and its impact on UX design processes, and discusses implications for LLM-based design methods and tools.},
	urldate = {2024-04-01},
	booktitle = {Proceedings of the 2023 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Zamfirescu-Pereira, J.D. and Wei, Heather and Xiao, Amy and Gu, Kitty and Jung, Grace and Lee, Matthew G and Hartmann, Bjoern and Yang, Qian},
	month = jul,
	year = {2023},
	keywords = {GPT., Prompt engineering, UX, conversational user interface},
	pages = {2206--2220},
}

@misc{kim_understanding_2024,
	title = {Understanding {Users}' {Dissatisfaction} with {ChatGPT} {Responses}: {Types}, {Resolving} {Tactics}, and the {Effect} of {Knowledge} {Level}},
	shorttitle = {Understanding {Users}' {Dissatisfaction} with {ChatGPT} {Responses}},
	url = {http://arxiv.org/abs/2311.07434},
	doi = {10.1145/3640543.3645148},
	abstract = {Large language models (LLMs) with chat-based capabilities, such as ChatGPT, are widely used in various workflows. However, due to a limited understanding of these large-scale models, users struggle to use this technology and experience different kinds of dissatisfaction. Researchers have introduced several methods, such as prompt engineering, to improve model responses. However, they focus on enhancing the model's performance in specific tasks, and little has been investigated on how to deal with the user dissatisfaction resulting from the model's responses. Therefore, with ChatGPT as the case study, we examine users' dissatisfaction along with their strategies to address the dissatisfaction. After organizing users' dissatisfaction with LLM into seven categories based on a literature review, we collected 511 instances of dissatisfactory ChatGPT responses from 107 users and their detailed recollections of dissatisfactory experiences, which we released as a publicly accessible dataset. Our analysis reveals that users most frequently experience dissatisfaction when ChatGPT fails to grasp their intentions, while they rate the severity of dissatisfaction related to accuracy the highest. We also identified four tactics users employ to address their dissatisfaction and their effectiveness. We found that users often do not use any tactics to address their dissatisfaction, and even when using tactics, 72\% of dissatisfaction remained unresolved. Moreover, we found that users with low knowledge of LLMs tend to face more dissatisfaction on accuracy while they often put minimal effort in addressing dissatisfaction. Based on these findings, we propose design implications for minimizing user dissatisfaction and enhancing the usability of chat-based LLM.},
	urldate = {2024-04-01},
	author = {Kim, Yoonsu and Lee, Jueon and Kim, Seoyoung and Park, Jaehyuk and Kim, Juho},
	month = mar,
	year = {2024},
	note = {arXiv:2311.07434 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@inproceedings{petridis_promptinfuser_2023,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {{PromptInfuser}: {Bringing} {User} {Interface} {Mock}-ups to {Life} with {Large} {Language} {Models}},
	isbn = {978-1-4503-9422-2},
	shorttitle = {{PromptInfuser}},
	url = {https://dl.acm.org/doi/10.1145/3544549.3585628},
	doi = {10.1145/3544549.3585628},
	abstract = {Large Language Models have enabled novices without machine learning (ML) experience to quickly prototype ML functionalities with prompt programming. This paper investigates incorporating prompt-based prototyping into designing functional user interface (UI) mock-ups. To understand how infusing LLM prompts into UI mock-ups might affect the prototyping process, we conduct a exploratory study with five designers, and find that this capability might significantly speed up creating functional prototypes, inform designers earlier on how their designs will integrate ML, and enable user studies with functional prototypes earlier. From these findings, we built PromptInfuser, a Figma plugin for authoring LLM-infused mock-ups. PromptInfuser introduces two novel LLM-interactions: input-output, which makes content interactive and dynamic, and frame-change, which directs users to different frames depending on their natural language input. From initial observations, we find that PromptInfuser has the potential to transform the design process by tightly integrating UI and AI prototyping in a single interface.},
	urldate = {2024-04-01},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Petridis, Savvas and Terry, Michael and Cai, Carrie Jun},
	month = apr,
	year = {2023},
	keywords = {Design, Generative AI, Large Language Models, Prototyping},
	pages = {1--6},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, {Prompt}, and {Predict}: {A} {Systematic} {Survey} of {Prompting} {Methods} in {Natural} {Language} {Processing}},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	doi = {10.48550/arXiv.2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{subramonyam_bridging_2024,
	title = {Bridging the {Gulf} of {Envisioning}: {Cognitive} {Design} {Challenges} in {LLM} {Interfaces}},
	shorttitle = {Bridging the {Gulf} of {Envisioning}},
	url = {http://arxiv.org/abs/2309.14459},
	abstract = {Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman's gulfs of execution and evaluation. To address this gap, we theorize how end-users 'envision' translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM's output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions.},
	urldate = {2024-04-01},
	publisher = {arXiv},
	author = {Subramonyam, Hariharan and Pea, Roy and Pondoc, Christopher Lawrence and Agrawala, Maneesh and Seifert, Colleen},
	month = mar,
	year = {2024},
	note = {arXiv:2309.14459 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	note = {Publisher: OpenAI},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2024-03-30},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{dey_gate-variants_2017,
	title = {Gate-variants of gated recurrent unit ({GRU}) neural networks},
	isbn = {1-5090-6389-7},
	publisher = {IEEE},
	author = {Dey, Rahul and Salem, Fathi M},
	year = {2017},
	pages = {1597--1600},
}

@article{khandelwal_sharp_2018,
	title = {Sharp nearby, fuzzy far away: {How} neural language models use context},
	journal = {arXiv preprint arXiv:1805.04623},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	year = {2018},
}

@article{graves_long_2012,
	title = {Long short-term memory},
	issn = {3642247962},
	journal = {Supervised sequence labelling with recurrent neural networks},
	author = {Graves, Alex and Graves, Alex},
	year = {2012},
	note = {Publisher: Springer},
	pages = {37--45},
}

@inproceedings{mikolov_recurrent_2010,
	title = {Recurrent neural network based language model.},
	volume = {2},
	publisher = {Makuhari},
	author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernocký, Jan and Khudanpur, Sanjeev},
	year = {2010},
	note = {Issue: 3},
	pages = {1045--1048},
}

@inproceedings{bengio_neural_2000,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	volume = {13},
	url = {https://proceedings.neurips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html},
	abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
	urldate = {2024-03-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal},
	year = {2000},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2024-03-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	year = {2017},
}

@article{reynolds_gaussian_2009,
	title = {Gaussian mixture models.},
	volume = {741},
	number = {659-663},
	journal = {Encyclopedia of biometrics},
	author = {Reynolds, Douglas A},
	year = {2009},
	note = {Publisher: Berlin, Springer},
}

@incollection{knill_hidden_1997,
	address = {Dordrecht},
	title = {Hidden {Markov} {Models} in {Speech} and {Language} {Processing}},
	isbn = {978-94-017-1183-8},
	url = {https://doi.org/10.1007/978-94-017-1183-8_2},
	abstract = {Speech is an acoustic representation of a word or sequence of words, characterized by a slowly changing spectral envelope. Humans perceive this spectral envelope, and convert it into the underlying word string and its associated meaning. The ultimate goal of speech and language processing is to mimic this process so that a machine can hold a natural conversation with a human. Speech and language processing has a far wider role to play, however, in performing less complex tasks such as transcription, language identification, or audio document retrieval, all of which are feasible to a certain extent now. The basic step in all of these systems is to perform the inverse mapping of the speech into the underlying sequence of symbols, usually words, that produced it, as shown in Fig. 2.1. This chapter describes a statistical approach to solving the automatic speech recognition problem, based on stochastic Markov process models.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {Corpus-{Based} {Methods} in {Language} and {Speech} {Processing}},
	publisher = {Springer Netherlands},
	author = {Knill, K. and Young, S.},
	editor = {Young, Steve and Bloothooft, Gerrit},
	year = {1997},
	doi = {10.1007/978-94-017-1183-8_2},
	keywords = {Acoustic Model, Hide Markov Model, Language Model, Speech Recognition, State Sequence},
	pages = {27--68},
}

@misc{cao_comprehensive_2023,
	title = {A {Comprehensive} {Survey} of {AI}-{Generated} {Content} ({AIGC}): {A} {History} of {Generative} {AI} from {GAN} to {ChatGPT}},
	shorttitle = {A {Comprehensive} {Survey} of {AI}-{Generated} {Content} ({AIGC})},
	url = {http://arxiv.org/abs/2303.04226},
	abstract = {YIHAN CAO∗, Lehigh University \& Carnegie Mellon University, USA SIYU LI, Lehigh University, USA YIXIN LIU, Lehigh University, USA ZHILING YAN, Lehigh University, USA YUTONG DAI, Lehigh University, USA PHILIP S. YU, University of Illinois at Chicago, USA LICHAO SUN, Lehigh University, USA Recently, ChatGPT, along with DALL-E-2 [1] and Codex [2],has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC. CCS Concepts: • Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability.},
	language = {en},
	urldate = {2024-03-30},
	publisher = {arXiv},
	author = {Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling and Dai, Yutong and Yu, Philip S. and Sun, Lichao},
	month = mar,
	year = {2023},
	note = {arXiv:2303.04226 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{feuerriegel_generative_2024,
	title = {Generative {AI}},
	volume = {66},
	issn = {1867-0202},
	url = {https://doi.org/10.1007/s12599-023-00834-7},
	doi = {10.1007/s12599-023-00834-7},
	language = {en},
	number = {1},
	urldate = {2024-03-30},
	journal = {Business \& Information Systems Engineering},
	author = {Feuerriegel, Stefan and Hartmann, Jochen and Janiesch, Christian and Zschech, Patrick},
	month = feb,
	year = {2024},
	keywords = {Artificial intelligence, Content creation, Decision support, Generative AI, Information systems},
	pages = {111--126},
}

@misc{klarna_klarna_2024,
	title = {Klarna {AI} assistant handles two-thirds of customer service chats in its first month {\textbar} {Klarna} {International}},
	url = {https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/},
	language = {en},
	urldate = {2024-03-30},
	author = {Klarna},
	year = {2024},
}

@misc{brynjolfsson_generative_2023,
	type = {Working {Paper}},
	series = {Working {Paper} {Series}},
	title = {Generative {AI} at {Work}},
	url = {https://www.nber.org/papers/w31161},
	doi = {10.3386/w31161},
	abstract = {New AI tools have the potential to change the way workers perform and learn, but little is known about their impacts on the job. In this paper, we study the staggered introduction of a generative AI-based conversational assistant using data from 5,179 customer support agents. Access to the tool increases productivity, as measured by issues resolved per hour, by 14\% on average, including a 34\% improvement for novice and low-skilled workers but with minimal impact on experienced and highly skilled workers. We provide suggestive evidence that the AI model disseminates the best practices of more able workers and helps newer workers move down the experience curve. In addition, we find that AI assistance improves customer sentiment, increases employee retention, and may lead to worker learning. Our results suggest that access to generative AI can increase productivity, with large heterogeneity in effects across workers.},
	urldate = {2024-03-30},
	publisher = {National Bureau of Economic Research},
	author = {Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey R.},
	month = apr,
	year = {2023},
	doi = {10.3386/w31161},
}

@misc{anthropic_introducing_2024,
	title = {Introducing the next generation of {Claude}},
	url = {https://www.anthropic.com/news/claude-3-family},
	abstract = {Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.},
	language = {en},
	urldate = {2024-03-29},
	journal = {Anthropic},
	author = {Anthropic},
	year = {2024},
}

@misc{malik_openais_2023,
	title = {{OpenAI}'s {ChatGPT} now has 100 million weekly active users},
	url = {https://techcrunch.com/2023/11/06/openais-chatgpt-now-has-100-million-weekly-active-users/},
	abstract = {ChatGPT now has 100 million weekly users, OpenAI CEO Sam Altman announced on Monday at the company's first developer conference.},
	language = {en-US},
	urldate = {2024-03-29},
	journal = {TechCrunch},
	author = {Malik, Aisha},
	month = nov,
	year = {2023},
}

@article{hu_chatgpt_2023,
	chapter = {Technology},
	title = {{ChatGPT} sets record for fastest-growing user base - analyst note},
	url = {https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/},
	abstract = {ChatGPT, the popular chatbot from OpenAI, is estimated to have reached 100 million monthly active users in January, just two months after launch, making it the fastest-growing consumer application in history, according to a UBS study on Wednesday.},
	language = {en},
	urldate = {2024-03-29},
	journal = {Reuters},
	author = {Hu, Krystal and Hu, Krystal},
	month = feb,
	year = {2023},
}

@article{brynjolfsson_productivity_1993,
	title = {The productivity paradox of information technology},
	volume = {36},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/163298.163309},
	doi = {10.1145/163298.163309},
	language = {en},
	number = {12},
	urldate = {2024-03-24},
	journal = {Communications of the ACM},
	author = {Brynjolfsson, Erik},
	month = dec,
	year = {1993},
	pages = {66--77},
}

@article{alavi_how_2023,
	title = {How {Generative} {AI} {Will} {Transform} {Knowledge} {Work}},
	issn = {0017-8012},
	url = {https://hbr.org/2023/11/how-generative-ai-will-transform-knowledge-work},
	abstract = {Generative AI can be a boon for knowledge work, but only if you use it in the right way. New generative AI-enabled tools are rapidly emerging to assist and transform knowledge work in industries ranging from education and finance to law and medicine. Companies are starting to introduce generative AI-powered innovations into their processes, and to promulgate policies on how to use the tools safely. However, there is no need to wait for these externally-imposed changes. You can start now to use generative AI for your own benefit, once your understand and learn to mitigate the associated risks. Using free tools already available on the web, you can reduce your cognitive load from constantly rising tides of information, while also boosting your cognitive abilities and learning effectiveness. Now is the time to start using generative AI in your knowledge work, and to help your colleagues to use it wisely.},
	urldate = {2024-03-24},
	journal = {Harvard Business Review},
	author = {Alavi, Maryam and Westerman, George},
	month = nov,
	year = {2023},
	note = {Section: Technology and analytics},
	keywords = {AI and machine learning, Algorithms, Analytics and data science, Automation, Data management, Enterprise computing, Information management, Technology and analytics},
}

@article{karr-wisniewski_when_2010,
	series = {Advancing {Educational} {Research} on {Computer}-supported {Collaborative} {Learning} ({CSCL}) through the use of {gStudy} {CSCL} {Tools}},
	title = {When more is too much: {Operationalizing} technology overload and exploring its impact on knowledge worker productivity},
	volume = {26},
	issn = {0747-5632},
	shorttitle = {When more is too much},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563210000488},
	doi = {10.1016/j.chb.2010.03.008},
	abstract = {Individuals within organizations are beginning to make an important realization: more information technology (IT) usage in the workplace can, at times, lead to productivity losses. We conceptualize this frequently observed, but largely ignored phenomenon as technology overload, when additional technology tools begin to crowd out one’s productivity instead of enhancing it. We found support for three main factors contributing technology-based productivity losses through information overload, communication overload, and system feature overload. Interestingly, these factors are a function of the individuals who use the technology, not the technology itself. In this paper, we present the results from three studies that (1) develop and pre-test a scale measurement for technology overload and its distinct dimensions, (2) validate the instrument, and (3) explore the relationship between technology overload and knowledge worker productivity. Our findings demonstrate the relationship between information technology usage and knowledge worker productivity, and they suggest how tradeoffs can be managed to ameliorate technology overload.},
	number = {5},
	urldate = {2024-03-24},
	journal = {Computers in Human Behavior},
	author = {Karr-Wisniewski, Pamela and Lu, Ying},
	month = sep,
	year = {2010},
	keywords = {Bounded rationality, Cognitive load theory, Communication overload, Human interruption theory, Information overload, Productivity, System feature overload, Technology overload},
	pages = {1061--1072},
}

@article{drucker_knowledge-worker_1999,
	title = {Knowledge-worker productivity: {The} biggest challenge},
	volume = {41},
	issn = {0008-1256},
	number = {2},
	journal = {California management review},
	author = {Drucker, Peter F},
	year = {1999},
	note = {Publisher: SAGE Publications Sage CA: Los Angeles, CA},
	pages = {79--94},
}

@inproceedings{amershi_guidelines_2019,
	address = {New York, NY, USA},
	series = {{CHI} '19},
	title = {Guidelines for {Human}-{AI} {Interaction}},
	isbn = {978-1-4503-5970-2},
	url = {https://doi.org/10.1145/3290605.3300233},
	doi = {10.1145/3290605.3300233},
	abstract = {Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the 2019 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
	month = may,
	year = {2019},
	keywords = {ai-infused systems, design guidelines, human-ai interaction},
	pages = {1--13},
}

@article{abedin_designing_2022,
	title = {Designing and {Managing} {Human}-{AI} {Interactions}},
	volume = {24},
	doi = {10.1007/s10796-022-10313-1},
	journal = {Information Systems Frontiers},
	author = {Abedin, Babak and Meske, Christian and Junglas, Iris and Rabhi, Fethi and Motahari-Nezhad, Hamid},
	month = jul,
	year = {2022},
}

@misc{yan_human-ai_2023,
	title = {Human-{AI} {Collaboration} in {Thematic} {Analysis} using {ChatGPT}: {A} {User} {Study} and {Design} {Recommendations}},
	shorttitle = {Human-{AI} {Collaboration} in {Thematic} {Analysis} using {ChatGPT}},
	url = {http://arxiv.org/abs/2311.03999},
	doi = {10.48550/arXiv.2311.03999},
	abstract = {Generative artificial intelligence (GenAI) offers promising potential for advancing human-AI collaboration in qualitative research. However, existing works focused on conventional machine-learning and pattern-based AI systems, and little is known about how researchers interact with GenAI in qualitative research. This work delves into researchers' perceptions of their collaboration with GenAI, specifically ChatGPT. Through a user study involving ten qualitative researchers, we found ChatGPT to be a valuable collaborator for thematic analysis, enhancing coding efficiency, aiding initial data exploration, offering granular quantitative insights, and assisting comprehension for non-native speakers and non-experts. Yet, concerns about its trustworthiness and accuracy, reliability and consistency, limited contextual understanding, and broader acceptance within the research community persist. We contribute five actionable design recommendations to foster effective human-AI collaboration. These include incorporating transparent explanatory mechanisms, enhancing interface and integration capabilities, prioritising contextual understanding and customisation, embedding human-AI feedback loops and iterative functionality, and strengthening trust through validation mechanisms.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Yan, Lixiang and Echeverria, Vanessa and Nieto, Gloria Fernandez and Jin, Yueqiao and Swiecki, Zachari and Zhao, Linxuan and Gašević, Dragan and Martinez-Maldonado, Roberto},
	month = nov,
	year = {2023},
	note = {arXiv:2311.03999 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@inproceedings{zamfirescu-pereira_why_2023,
	address = {New York, NY, USA},
	series = {{CHI} '23},
	title = {Why {Johnny} {Can}’t {Prompt}: {How} {Non}-{AI} {Experts} {Try} (and {Fail}) to {Design} {LLM} {Prompts}},
	isbn = {978-1-4503-9421-5},
	shorttitle = {Why {Johnny} {Can}’t {Prompt}},
	url = {https://dl.acm.org/doi/10.1145/3544548.3581388},
	doi = {10.1145/3544548.3581388},
	abstract = {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fluent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting effective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to effective prompt design. These findings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
	month = apr,
	year = {2023},
	keywords = {design tools, end-users, language models},
	pages = {1--21},
}

@inproceedings{cabrero-daniel_perceived_2023,
	address = {New York, NY, USA},
	series = {{TAS} '23},
	title = {Perceived {Trustworthiness} of {Natural} {Language} {Generators}},
	isbn = {9798400707346},
	url = {https://dl.acm.org/doi/10.1145/3597512.3599715},
	doi = {10.1145/3597512.3599715},
	abstract = {Natural Language Generation tools, such as chatbots that can generate human-like conversational text, are becoming more common both for personal and professional use. However, there are concerns about their trustworthiness and ethical implications. The paper addresses the problem of understanding how different users (e.g., linguists, engineers) perceive and adopt these tools and their perception of machine-generated text quality. It also discusses the perceived advantages and limitations of Natural Language Generation tools, as well as users’ beliefs on governance strategies. The main findings of this study include the impact of users’ field and level of expertise on the perceived trust and adoption of Natural Language Generation tools, the users’ assessment of the accuracy, fluency, and potential biases of machine-generated text in comparison to human-written text, and an analysis of the advantages and ethical risks associated with these tools as identified by the participants. Moreover, this paper discusses the potential implications of these findings for enhancing the AI development process. The paper sheds light on how different user characteristics shape their beliefs on the quality and overall trustworthiness of machine-generated text. Furthermore, it examines the benefits and risks of these tools from the perspectives of different users.},
	urldate = {2024-03-24},
	booktitle = {Proceedings of the {First} {International} {Symposium} on {Trustworthy} {Autonomous} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Cabrero-Daniel, Beatriz and Sanagustín Cabrero, Andrea},
	month = jul,
	year = {2023},
	keywords = {Natural Language Generation, human computer interaction, perceived limitations, trustworthy artificial intelligence},
	pages = {1--9},
}

@misc{chen_next_2023,
	title = {Next {Steps} for {Human}-{Centered} {Generative} {AI}: {A} {Technical} {Perspective}},
	shorttitle = {Next {Steps} for {Human}-{Centered} {Generative} {AI}},
	url = {http://arxiv.org/abs/2306.15774},
	doi = {10.48550/arXiv.2306.15774},
	abstract = {Through iterative, cross-disciplinary discussions, we define and propose next-steps for Human-centered Generative AI (HGAI). We contribute a comprehensive research agenda that lays out future directions of Generative AI spanning three levels: aligning with human values; assimilating human intents; and augmenting human abilities. By identifying these next-steps, we intend to draw interdisciplinary research teams to pursue a coherent set of emergent ideas in HGAI, focusing on their interested topics while maintaining a coherent big picture of the future work landscape.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Chen, Xiang 'Anthony' and Burke, Jeff and Du, Ruofei and Hong, Matthew K. and Jacobs, Jennifer and Laban, Philippe and Li, Dingzeyu and Peng, Nanyun and Willis, Karl D. D. and Wu, Chien-Sheng and Zhou, Bolei},
	month = dec,
	year = {2023},
	note = {arXiv:2306.15774 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{tankelevitch_metacognitive_2024,
	title = {The {Metacognitive} {Demands} and {Opportunities} of {Generative} {AI}},
	url = {http://arxiv.org/abs/2312.10893},
	abstract = {Generative AI (GenAI) systems offer unprecedented opportunities for transforming professional and personal work, yet present challenges around prompting, evaluating and relying on outputs, and optimizing workflows. We argue that metacognition\${\textbackslash}unicode\{x2013\}\$the psychological ability to monitor and control one's thoughts and behavior\${\textbackslash}unicode\{x2013\}\$offers a valuable lens to understand and design for these usability challenges. Drawing on research in psychology and cognitive science, and recent GenAI user studies, we illustrate how GenAI systems impose metacognitive demands on users, requiring a high degree of metacognitive monitoring and control. We propose these demands could be addressed by integrating metacognitive support strategies into GenAI systems, and by designing GenAI systems to reduce their metacognitive demand by targeting explainability and customizability. Metacognition offers a coherent framework for understanding the usability challenges posed by GenAI, and provides novel research and design directions to advance human-AI interaction.},
	urldate = {2024-03-24},
	publisher = {arXiv},
	author = {Tankelevitch, Lev and Kewenig, Viktor and Simkute, Auste and Scott, Ava Elizabeth and Sarkar, Advait and Sellen, Abigail and Rintel, Sean},
	month = mar,
	year = {2024},
	note = {arXiv:2312.10893 [cs]},
	keywords = {Computer Science - Human-Computer Interaction},
}
